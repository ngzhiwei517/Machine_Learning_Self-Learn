{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2oERiwF3gtCG1ZFfsOF2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/Machine_Learning_Self-Learn/blob/main/Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Mining"
      ],
      "metadata": {
        "id": "msEfCXlgVIPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Lake dataset\n",
        "\n",
        "After surfing through the Internet, we found that we were unable to find any historical data about the water level of the dam in Malaysia. The most we can get is the current data which is too little to train our model even if we start collecting it by April 2024. Hence, we decided to look for data out of Malaysia.\n",
        "\n",
        "Luckily for us, we were able to find a website which contain the historical data regarding a few lakes located in Phoenix, AZ, USA. In order to extract those data out and store it in a .csv file, we have to perform Web Scrapping.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wxl7vuCwVJrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Inspecting the website (Lake dataset)"
      ],
      "metadata": {
        "id": "J31OreQNVdfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before scrapping the data out from the website, we make some discovering about the website first and understand where the data is stored and how to extract the data for a period of time by performing the following code.\n",
        "\n",
        "We need the help of some packages in order to perform web scrapping.\n",
        "\n",
        "* BeautifulSoup\n",
        "* requests"
      ],
      "metadata": {
        "id": "Qc9-qaHiVgRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "QYhAxTVJVfeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we try to get request from the website.\n",
        "\n",
        "Source: https://www.watershedconnection.com/"
      ],
      "metadata": {
        "id": "AKBnSVtwVknR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Website for data mining\n",
        "url = \"https://streamflow.watershedconnection.com/DWR?reportDate=2017-1-1\""
      ],
      "metadata": {
        "id": "4SZA7UqpVqNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get requests from website\n",
        "requests.get(url)"
      ],
      "metadata": {
        "id": "P_SCiiF_VsE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It returns us a response 200. Response 200 indicates that our request to the website has succeeded.\n",
        "\n",
        "Now, we try to get the HTML of the website."
      ],
      "metadata": {
        "id": "qqzJYfKlVta6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get requests from website\n",
        "page_lake_test = requests.get(url)"
      ],
      "metadata": {
        "id": "lL9I2og4VzVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the html of the website\n",
        "soup_lake_test = BeautifulSoup(page_lake_test.text, 'html')"
      ],
      "metadata": {
        "id": "tY0NnYQEV0d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the html of the website\n",
        "print(soup_lake_test.prettify())"
      ],
      "metadata": {
        "id": "jYSKx23gV2w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, there is quite a lot of code in the HTML. By inspecting the website, we know that the table containing the data is code under the tags 'table'. Let's have a search for the tags 'table'."
      ],
      "metadata": {
        "id": "3WusCmX0V3R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all 'table' in the html (Searching for the tags containing the table of the data)\n",
        "soup_lake_test.find_all('td')"
      ],
      "metadata": {
        "id": "3fLH4XdsV4Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is the tags 'table' containing the data we want. However, there are also other tables in that website too. We select the table we want by adding a '[0]' at the end of the previous code."
      ],
      "metadata": {
        "id": "rMOgDsI1V6Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all 'table' in the html (Searching for the tags containing the table of the data)\n",
        "soup_lake_test.find_all('table')[0]"
      ],
      "metadata": {
        "id": "OUrzqAP3V8vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table we wanted has been selected. From the code, we able to see that the data we wanted is under the tags 'td'. Let's try to get the data out."
      ],
      "metadata": {
        "id": "WnZhWWj7V9_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all 'td' in the html (Searching for the data we wanted)\n",
        "soup_lake_test.find_all('td')"
      ],
      "metadata": {
        "id": "IjADUhLOWAI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's pull the first data out from the tags to see whether the data is in the format that we wanted."
      ],
      "metadata": {
        "id": "jtTEkSUHWCb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling text information from the website\n",
        "soup_lake_test.find('td').text"
      ],
      "metadata": {
        "id": "zEeZ9FA6WCxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, almost there but it containing something that we don't want. Let's trim it off."
      ],
      "metadata": {
        "id": "vWxE6kLSWEdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling text information from the website\n",
        "soup_lake_test.find('td').text.strip()"
      ],
      "metadata": {
        "id": "3H-jqr0yWFZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice, this is what we wanted."
      ],
      "metadata": {
        "id": "fxAFKzNBr9AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The websites also containing some information of the weather for that respective day. From the inspection of the websites, the weather data is code under the 'b' tags."
      ],
      "metadata": {
        "id": "BkXUsmy0sCRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all 'b' in the html (Searching for the weather data)\n",
        "soup_lake_test.find_all('b')"
      ],
      "metadata": {
        "id": "7_pSyVAPWIf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each data is in the format highest/lowest. So, let us try to separate the first data."
      ],
      "metadata": {
        "id": "9pIIxeC3WJZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the data and removing unwanted value from the data\n",
        "temperature_test = soup_lake_test.find_all('b')[0]\n",
        "temperature_list_test = temperature_test.text.strip().split(\"°\")\n",
        "temperature_list_test[1] = temperature_list_test[1].replace(\"/\", \"\")\n",
        "temperature_list_test"
      ],
      "metadata": {
        "id": "Lhp5AEezWLT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above list, we know that the data one is the first and the second items in the list.\n",
        "\n",
        "Since we know where our data is located in the website, it's time to extract those data and store it in a dataframe."
      ],
      "metadata": {
        "id": "9qCHWrrsWM-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the table needed\n",
        "table_lake_test = soup_lake_test.find_all('table')[0]"
      ],
      "metadata": {
        "id": "8mGghs3oWOrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before creating the dataframe, we need to set the feature names first. The feature names are under the tags 'th'. Let's have a look of it."
      ],
      "metadata": {
        "id": "FSjol9zxWPh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all 'th' in the html (Searching for the feature names)\n",
        "soup_lake_test.find_all('th')"
      ],
      "metadata": {
        "id": "N1U3-oTsWP2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was an unexpected coding. We have no choice to set the feature names ourself."
      ],
      "metadata": {
        "id": "VDVTKuwHWR_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the feature names manually into a list\n",
        "feature_names_lake_test = ['Location', 'Full (%)', 'Current Elevation(ft)',\n",
        "                      'Current Storage (af)', 'Remaining Elevation (ft)',\n",
        "                      'Available Storage (af)', '24 hr. Change',\n",
        "                      'Rain (inches)']\n",
        "feature_names_lake_test"
      ],
      "metadata": {
        "id": "tUkpTnqIWS2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better. Now we will create a dataframe and set the feature names into it."
      ],
      "metadata": {
        "id": "kX_PBUlhWTyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ukoQjPsVWUS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lake_test_df = pd.DataFrame(columns = feature_names_lake_test)\n",
        "lake_test_df"
      ],
      "metadata": {
        "id": "FqntaZKEhjND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to abtract the data we wanted and storing it into the dataframe."
      ],
      "metadata": {
        "id": "r3MORhWpvwpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_data_lake_test = table_lake_test.find_all('tr')\n",
        "for row in column_data_lake_test[2:]:\n",
        "  row_data = row.find_all('td')\n",
        "  individual_row_data = [data.text.strip() for data in row_data]\n",
        "\n",
        "  length = len(lake_test_df)\n",
        "  lake_test_df.loc[length] = individual_row_data\n",
        "\n",
        "lake_test_df"
      ],
      "metadata": {
        "id": "59ZkvxUvhj3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like the web scrapping works for this website as the data we wanted is successfully extract from the website into the dataframe. Now, we will be moving to the next step which is extracting data for a range of time.\n",
        "\n"
      ],
      "metadata": {
        "id": "blFEB8Z0ho6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Extracting data for a range of time (Lake dataset)"
      ],
      "metadata": {
        "id": "H5ggkzRChp-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the date range for the data we wanted to extract."
      ],
      "metadata": {
        "id": "7QE2aRckh0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Date range\n",
        "date_start_text = \"2013-1-1\"\n",
        "date_end_text = \"2023-1-1\"\n",
        "freq_text = \"1\""
      ],
      "metadata": {
        "id": "lfJUryYyh0PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The range of our data gathering will be daily for a period of 10 years."
      ],
      "metadata": {
        "id": "umhfEJWgqS9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convertion from text to date is necessary to be put as the range of the while loop."
      ],
      "metadata": {
        "id": "tiiT5UH9q7ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to date\n",
        "date_start_inc = date_start_text\n",
        "date_start_inc_date = datetime.strptime(date_start_inc, \"%Y-%m-%d\")\n",
        "date_end_date = datetime.strptime(date_end_text, \"%Y-%m-%d\")\n",
        "freq_obj = int(freq_text)\n",
        "\n",
        "date_start_inc_date = date_start_inc_date.date()\n",
        "date_end_date = date_end_date.date()"
      ],
      "metadata": {
        "id": "_Raz75SGh3pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up features name\n",
        "feature_names_lake = [ 'Location', 'Full_%', 'Current_Elevation_ft',\n",
        "                      'Current_Storage_af', 'Remaining_Elevation_ft',\n",
        "                      'Available_Storage_af', '24hr.Change',\n",
        "                      'Rain_inches', 'Date', 'Highest_temperature',\n",
        "                       'Lowest_temperature','Highest_humidity',\n",
        "                       'Lowest_humidity']\n",
        "feature_names_lake"
      ],
      "metadata": {
        "id": "7dwnVa9sh661"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the dataframe to store data later with the feature names we set previously\n",
        "lake_df = pd.DataFrame(columns = feature_names_lake)"
      ],
      "metadata": {
        "id": "Aw5HT0hJh8ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataframe is ready to store data gather from the website"
      ],
      "metadata": {
        "id": "HccHpOy9h-pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the while loop to gather data for a period of time\n",
        "while date_start_inc_date <= date_end_date:\n",
        "  # Date will be automatically insert to the back of the url to access data for that particular day\n",
        "  url = 'https://streamflow.watershedconnection.com/DWR?reportDate='+ date_start_inc_date.strftime(\"%Y-%m-%d\")\n",
        "  page_lake = requests.get(url)\n",
        "  soup_lake = BeautifulSoup(page_lake.text, 'html')\n",
        "  table_lake = soup_lake.find_all('table')[0]\n",
        "  column_data_lake = table_lake.find_all('tr')\n",
        "  for row_lake in column_data_lake[2:]:\n",
        "    row_data_lake = row_lake.find_all('td')\n",
        "    individual_row_data_lake = [data.text.strip() for data in row_data_lake]\n",
        "    individual_row_data_lake.append(date_start_inc_date.strftime(\"%Y-%m-%d\"))\n",
        "    # Triming the temperature value and adding it into the list\n",
        "    temperature = soup_lake.find_all('b')[0]\n",
        "    temperature_list = temperature.text.strip().split(\"°\")\n",
        "    temperature_list[1] = temperature_list[1].replace(\"/\", \"\")\n",
        "    individual_row_data_lake.append(temperature_list[0])\n",
        "    individual_row_data_lake.append(temperature_list[1])\n",
        "    # Triming the humidity value and adding it into the list\n",
        "    humidity = soup_lake.find_all('b')[2]\n",
        "    humidity_list = humidity.text.strip().split()\n",
        "    individual_row_data_lake.append(humidity_list[0])\n",
        "    individual_row_data_lake.append(humidity_list[2])\n",
        "    # Storing the list of data into the dataframe\n",
        "    length = len(lake_df)\n",
        "    lake_df.loc[length] = individual_row_data_lake\n",
        "\n",
        "  date_start_inc_date = date_start_inc_date + timedelta(days = freq_obj)"
      ],
      "metadata": {
        "id": "eO8lX4Huh_QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the table above, the data we gather has correctly store in their respective column. A total of 36530 rows of data gather for 10 years period"
      ],
      "metadata": {
        "id": "KCnOEYNhuaqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lake_df.to_csv(r'/content/sample_data/lake_dataset_10years.csv', index = False)"
      ],
      "metadata": {
        "id": "r44yU8reiGgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the data gather into a csv file.\n",
        "\n",
        "After inspecting the dataset, we found out the weather information gather from that website is not precise enough. Hence, we decided to gather the information of the weather from the nearest weather station to the area where the lakes are located from other website."
      ],
      "metadata": {
        "id": "r_d6F8CpiIzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Weather dataset"
      ],
      "metadata": {
        "id": "EkXtc2lWiSm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to see which weather station is the nearest to that area. Only the weather station at the airport in that area is storing the historical weather data.\n",
        "\n",
        "The airport available there is as follow:\n",
        "\n",
        "* Phoenix Sky Harbor International Airport\n",
        "* Phoenix-Mesa Gateway Airport\n",
        "* Falcon Field Airport\n",
        "* Scottsdale Airport\n",
        "* Phoenix Deer Valley Airport\n",
        "* Glendale Municipal Airport\n",
        "* Phoenix-Goodyear Airport\n",
        "\n",
        "We have chosen Falcon Field Airport since it is nearest to that area.\n",
        "\n",
        "We now gather the data about the weather information of that area from this website.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Source: https://www.visualcrossing.com/"
      ],
      "metadata": {
        "id": "ReJqlMR5iLI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 Inspecting the website (Weather dataset)\n",
        "\n",
        "After taking a tour through this website, it's provide API that allow user to get the data from their website easily. However, for free user, a total of 1000 records are allowed for a day. This mean that we need a few day to gather all the data we wanted.\n",
        "\n",
        "Since API service is provided, it save up a lot of our works to inspect the website.\n",
        "\n"
      ],
      "metadata": {
        "id": "OqZdRkwciYE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.3 Extracting data for a range of time (Weather dataset)"
      ],
      "metadata": {
        "id": "K2p7peStiem0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As same as previous, we first need to indicate the range of date of the data we wanted to extract. Since we are only able to extract limited data per day, in this notebook, we will only be extracting data for a period of one year."
      ],
      "metadata": {
        "id": "NRMv2c2oiiR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Date range\n",
        "date_start_text = \"2013-1-1\"\n",
        "date_end_text = \"2014-1-1\"\n",
        "freq_text = \"1\""
      ],
      "metadata": {
        "id": "7KI10WA7iJX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to date\n",
        "date_start_inc = date_start_text\n",
        "date_start_inc_date = datetime.strptime(date_start_inc, \"%Y-%m-%d\")\n",
        "date_end_date = datetime.strptime(date_end_text, \"%Y-%m-%d\")\n",
        "freq_obj = int(freq_text)\n",
        "\n",
        "date_start_inc_date = date_start_inc_date.date()\n",
        "date_end_date = date_end_date.date()"
      ],
      "metadata": {
        "id": "IX1Uz9DNikoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now start to extract the data we wanted using the API. We will be getting weather information for a day for each query we make. Hence, we need to create a list to store all the dataframe we get so that we can combine it into one later."
      ],
      "metadata": {
        "id": "oRqvSZiHimAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the list to store the .csv file\n",
        "list_of_csv = []\n",
        "# Creating the while loop\n",
        "while date_start_inc_date <= date_end_date:\n",
        "  url = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/%20Falcon%20Field%20Airport%2C%204800%20E%20Falcon%20Dr%2C%20Mesa/'+date_start_inc_date.strftime(\"%Y-%m-%d\")+'/' +date_start_inc_date.strftime(\"%Y-%m-%d\")+'?unitGroup=metric&include=days&key={API KEY}&contentType=csv'\n",
        "  df = pd.read_csv(url, index_col=None, header = 0)\n",
        "  date_start_inc_date = date_start_inc_date + timedelta(days = freq_obj)\n",
        "  # Adding the dataframe into the list\n",
        "  list_of_csv.append(df)"
      ],
      "metadata": {
        "id": "6hV4jbj2immj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all the dataframe in the list into one\n",
        "weather_2013_df = pd.concat(list_of_csv, axis= 0,ignore_index=True)"
      ],
      "metadata": {
        "id": "rH1Os-DWioda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the dataframe into a .csv file\n",
        "weather_2013_df.to_csv(r'/content/sample_data/weather_dataset_2013.csv', index = False)"
      ],
      "metadata": {
        "id": "RjNYU4usiqKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have the weather dataset for the year 2013. The dataset for the remaining years will be done using another notebook by using the same code as above"
      ],
      "metadata": {
        "id": "d18FB0OSippn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since all of the dataset we wanted are ready, it's time to proceed to the next section,which is Data Preprocessing."
      ],
      "metadata": {
        "id": "k0sV9gz6itWQ"
      }
    }
  ]
}